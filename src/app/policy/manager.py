import os
import json
import logging
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class PolicyManager:
    """Load and search policy text with lightweight and semantic capabilities."""

    def __init__(self, policy_file='data/policy.json', model_name='paraphrase-multilingual-MiniLM-L12-v2', lazy_load=True):
        self.policy_file = policy_file
        self.policy_data: Dict[str, Any] = {}
        self.policy_sentences: List[str] = []
        self.policy_embeddings = None
        self.model_name = model_name
        self.model: Optional[Any] = None  # SentenceTransformer model (heavy)
        self.lightweight_manager = None  # Lightweight policy manager
        self.lazy_load = lazy_load
        self._model_loaded = False

        # æ€»æ˜¯åŠ è½½æ”¿ç­–æ•°æ®ï¼ˆè¿™å¾ˆå¿«ï¼‰
        self.load_policy()

        # ä¼˜å…ˆåˆå§‹åŒ–è½»é‡çº§ç®¡ç†å™¨
        self._init_lightweight_manager()

        # æ ¹æ®lazy_loadå†³å®šæ˜¯å¦ç«‹å³åŠ è½½é‡å‹æ¨¡å‹
        if not lazy_load:
            self._load_model()
            self._generate_embeddings()

    def _init_lightweight_manager(self):
        """åˆå§‹åŒ–è½»é‡çº§æ”¿ç­–ç®¡ç†å™¨"""
        try:
            from .lightweight_manager import LightweightPolicyManager
            self.lightweight_manager = LightweightPolicyManager(
                policy_file=self.policy_file,
                lazy_load=True
            )
            logger.info("è½»é‡çº§æ”¿ç­–ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"è½»é‡çº§æ”¿ç­–ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.lightweight_manager = None

    def load_policy(self):
        """Loads policy data from the JSON file."""
        # Removed path modification logic, self.policy_file is expected to be correct
            
        logger.info(f"Attempting to load policy from: {self.policy_file}")

        try:
            with open(self.policy_file, 'r', encoding='utf-8') as f:
                self.policy_data = json.load(f)
            logger.info(f"Policy loaded successfully from {self.policy_file}")
            
            # Extract sentences for embedding
            self.policy_sentences = []
            if 'sections' in self.policy_data:
                for section_key, sentences in self.policy_data['sections'].items():
                    if isinstance(sentences, list):
                        self.policy_sentences.extend(sentences)
            logger.info(f"Extracted {len(self.policy_sentences)} sentences for embedding.")

        except FileNotFoundError:
            logger.error(f"Policy file {self.policy_file} not found.")
            self.policy_data = {}
            self.policy_sentences = []
        except json.JSONDecodeError:
            logger.error(f"Error decoding JSON from policy file {self.policy_file}.")
            self.policy_data = {}
            self.policy_sentences = []
        except Exception as e:
            logger.error(f"An unexpected error occurred while loading policy: {e}")
            self.policy_data = {}
            self.policy_sentences = []

    def _ensure_model_loaded(self):
        """ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if not self._model_loaded:
            self._load_model()
            self._generate_embeddings()
            self._model_loaded = True

    def _load_model(self):
        """Loads the sentence transformer model (heavy dependency)."""
        try:
            # æ‡’åŠ è½½é‡å‹ä¾èµ–
            from sentence_transformers import SentenceTransformer

            logger.info(f"æ­£åœ¨åŠ è½½SentenceTransformeræ¨¡å‹ '{self.model_name}'...")
            self.model = SentenceTransformer(self.model_name)
            logger.info(f"SentenceTransformer model '{self.model_name}' loaded successfully.")
        except ImportError:
            logger.warning("sentence-transformersæœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨è½»é‡çº§æ”¿ç­–æœç´¢")
            self.model = None
        except Exception as e:
            logger.error(f"Failed to load SentenceTransformer model '{self.model_name}': {e}")
            self.model = None

    def _generate_embeddings(self):
        """Generates embeddings for all policy sentences."""
        if self.model and self.policy_sentences:
            try:
                self.policy_embeddings = self.model.encode(self.policy_sentences, convert_to_tensor=True)
                logger.info(f"Generated embeddings for {len(self.policy_sentences)} policy sentences.")
            except Exception as e:
                logger.error(f"Failed to generate policy embeddings: {e}")
                self.policy_embeddings = None
        else:
            logger.warning("Skipping embedding generation: model not loaded or no sentences found.")
            self.policy_embeddings = None

    def find_policy_excerpt_semantic(self, query: str, top_k: int = 3) -> List[str]:
        """
        Finds policy excerpts semantically similar to the query.

        Args:
            query (str): The user's query.
            top_k (int): The number of top similar sentences to return.

        Returns:
            List[str]: A list of the most relevant policy sentences.
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆæ‡’åŠ è½½ï¼‰
        if self.lazy_load:
            self._ensure_model_loaded()

        if not self.model or self.policy_embeddings is None:
            logger.warning("Semantic search not available: model or embeddings missing.")
            # Fallback to keyword search if semantic search is not available
            keyword_result = self.find_policy_excerpt([query]) # Use the old method as fallback
            return [keyword_result] if keyword_result else []

        try:
            # æ‡’åŠ è½½é‡å‹ä¾èµ–
            from sentence_transformers import util
            import torch

            query_embedding = self.model.encode(query, convert_to_tensor=True)
            # Compute cosine-similarity between the query and all policy sentences
            cosine_scores = util.cos_sim(query_embedding, self.policy_embeddings)[0]

            # Find the top_k scores
            top_results = torch.topk(cosine_scores, k=min(top_k, len(self.policy_sentences)))

            relevant_sentences = []
            for score, idx in zip(top_results[0], top_results[1]):
                # Optional: Add a similarity threshold if needed
                # if score.item() > 0.5:
                relevant_sentences.append(self.policy_sentences[idx.item()])
                logger.debug(f"Semantic match: Score {score.item():.4f}, Sentence: {self.policy_sentences[idx.item()]}")

            return relevant_sentences

        except ImportError:
            logger.warning("é‡å‹ä¾èµ–æœªå®‰è£…ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢")
            keyword_result = self.find_policy_excerpt([query])
            return [keyword_result] if keyword_result else []
        except Exception as e:
            logger.error(f"An error occurred during semantic search: {e}")
            # Fallback to keyword search on error
            keyword_result = self.find_policy_excerpt([query]) # Use the old method as fallback
            return [keyword_result] if keyword_result else []

    def search_policy(self, query: str, top_k: int = 3) -> List[str]:
        """
        æœç´¢æ”¿ç­–ï¼Œä¼˜å…ˆä½¿ç”¨è½»é‡çº§æ–¹æ³•ï¼Œé‡å‹æ–¹æ³•ä½œä¸ºå¤‡é€‰

        Args:
            query (str): æœç´¢æŸ¥è¯¢
            top_k (int): è¿”å›ç»“æœæ•°é‡

        Returns:
            List[str]: ç›¸å…³æ”¿ç­–å¥å­åˆ—è¡¨
        """
        # ä¼˜å…ˆä½¿ç”¨è½»é‡çº§ç®¡ç†å™¨
        if self.lightweight_manager:
            try:
                results = self.lightweight_manager.search_policy(query, top_k)
                if results:
                    logger.debug(f"è½»é‡çº§æ”¿ç­–æœç´¢æ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                    return results
            except Exception as e:
                logger.warning(f"è½»é‡çº§æ”¿ç­–æœç´¢å¤±è´¥: {e}ï¼Œå›é€€åˆ°è¯­ä¹‰æœç´¢")

        # å›é€€åˆ°é‡å‹è¯­ä¹‰æœç´¢
        try:
            results = self.find_policy_excerpt_semantic(query, top_k)
            if results:
                logger.debug(f"è¯­ä¹‰æ”¿ç­–æœç´¢æ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                return results
        except Exception as e:
            logger.warning(f"è¯­ä¹‰æ”¿ç­–æœç´¢å¤±è´¥: {e}ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢")

        # æœ€åå›é€€åˆ°å…³é”®è¯æœç´¢
        keyword_result = self.find_policy_excerpt([query])
        return [keyword_result] if keyword_result else ["è¯·è”ç³»å®¢æœäº†è§£å…·ä½“æ”¿ç­–ä¿¡æ¯ã€‚"]

    def get_policy_categories(self) -> List[Dict[str, str]]:
        """
        è·å–æ”¿ç­–ç±»åˆ«åˆ—è¡¨ï¼Œç”¨äºç”Ÿæˆæ”¿ç­–é€‰æ‹©æŒ‰é’®

        Returns:
            List[Dict[str, str]]: æ”¿ç­–ç±»åˆ«åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«display_textå’Œpayload
        """
        # æ”¿ç­–ç±»åˆ«æ˜ å°„ï¼šsection_key -> ç”¨æˆ·å‹å¥½çš„æ˜¾ç¤ºåç§°
        category_mapping = {
            'mission': 'ç†å¿µå®—æ—¨',
            'group_rules': 'ç¾¤è§„åˆ¶åº¦',
            'product_quality': 'è´¨é‡ä¿è¯',
            'delivery': 'é…é€æ”¿ç­–',
            'payment': 'ä»˜æ¬¾æ–¹å¼',
            'pickup': 'å–è´§ä¿¡æ¯',
            'after_sale': 'å”®åæœåŠ¡',
            'community': 'ç¤¾åŒºäº’åŠ©'
        }

        categories = []
        if 'sections' in self.policy_data:
            for section_key, display_name in category_mapping.items():
                if section_key in self.policy_data['sections']:
                    categories.append({
                        'display_text': display_name,
                        'payload': f'policy_category:{section_key}'
                    })

        return categories

    def get_policy_by_category(self, category_key: str) -> str:
        """
        æ ¹æ®ç±»åˆ«é”®è·å–å…·ä½“æ”¿ç­–å†…å®¹

        Args:
            category_key (str): æ”¿ç­–ç±»åˆ«é”®ï¼ˆå¦‚'delivery', 'payment'ç­‰ï¼‰

        Returns:
            str: æ ¼å¼åŒ–çš„æ”¿ç­–å†…å®¹
        """
        # ç±»åˆ«åç§°æ˜ å°„
        category_names = {
            'mission': 'ç†å¿µå®—æ—¨',
            'group_rules': 'ç¾¤è§„åˆ¶åº¦',
            'product_quality': 'è´¨é‡ä¿è¯',
            'delivery': 'é…é€æ”¿ç­–',
            'payment': 'ä»˜æ¬¾æ–¹å¼',
            'pickup': 'å–è´§ä¿¡æ¯',
            'after_sale': 'å”®åæœåŠ¡',
            'community': 'ç¤¾åŒºäº’åŠ©'
        }

        if 'sections' not in self.policy_data or category_key not in self.policy_data['sections']:
            return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº{category_names.get(category_key, category_key)}çš„æ”¿ç­–ä¿¡æ¯ã€‚"

        category_name = category_names.get(category_key, category_key)
        policy_items = self.policy_data['sections'][category_key]

        if not policy_items:
            return f"æŠ±æ­‰ï¼Œ{category_name}çš„æ”¿ç­–ä¿¡æ¯æš‚æ—¶ä¸ºç©ºã€‚"

        # æ ¼å¼åŒ–æ”¿ç­–å†…å®¹
        response_parts = [f"ğŸ“‹ {category_name}"]
        response_parts.append("")  # ç©ºè¡Œ

        for i, item in enumerate(policy_items, 1):
            response_parts.append(f"â€¢ {item}")

        # æ·»åŠ ç‰ˆæœ¬ä¿¡æ¯
        version = self.get_policy_version()
        last_updated = self.get_policy_last_updated()
        response_parts.append("")
        response_parts.append(f"(æ”¿ç­–ç‰ˆæœ¬: {version}, æœ€åæ›´æ–°: {last_updated})")

        # æ·»åŠ å¼•å¯¼ä¿¡æ¯
        response_parts.append("")
        response_parts.append("å¦‚éœ€äº†è§£å…¶ä»–æ”¿ç­–ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥è¯¢é—®\"ä½ ä»¬æœ‰ä»€ä¹ˆæ”¿ç­–ï¼Ÿ\"æŸ¥çœ‹å®Œæ•´æ”¿ç­–åˆ—è¡¨ã€‚")

        return "\n".join(response_parts)

    def find_policy_excerpt(self, keywords: List[str]) -> str:
        """
        (Legacy) Return the first matching line using keywords priority from original lines.
        This method is kept for compatibility or fallback.
        """
        # Note: This method now operates on the extracted sentences from JSON,
        # not the original policy.md lines, unless load_policy failed.
        # If load_policy failed, self.policy_sentences will be empty,
        # and this method will return ''.
        
        # If policy_data was loaded, use the extracted sentences
        if self.policy_sentences:
             for kw in keywords:
                for sentence in self.policy_sentences:
                    if kw.lower() in sentence.lower():
                        return sentence
        # If policy_data failed to load, self.policy_sentences is empty,
        # so the loops above won't run. Return empty string.
        return ''

    def get_policy_version(self) -> str:
        """Returns the policy version."""
        return self.policy_data.get('version', 'N/A')

    def get_policy_last_updated(self) -> str:
        """Returns the last updated date of the policy."""
        return self.policy_data.get('last_updated', 'N/A')

    def get_policy_section(self, section_key: str) -> List[str]:
        """Returns sentences from a specific policy section."""
        return self.policy_data.get('sections', {}).get(section_key, [])

    def get_all_sections(self) -> List[str]:
        """Returns a list of all available policy section keys."""
        return list(self.policy_data.get('sections', {}).keys())

# Example Usage (for testing)
if __name__ == '__main__':
    # Assuming policy.json is in the same directory (or data/policy.json if run from root)
    # For this example to run standalone from src/app/policy, it would need policy_file='../../../data/policy.json'
    # Or, more robustly, configure the path based on expected execution context.
    # For now, let's assume it's run where 'data/policy.json' is accessible.
    policy_manager = PolicyManager(policy_file='data/policy.json') # Explicitly set for clarity if run from root

    print(f"Policy Version: {policy_manager.get_policy_version()}")
    print(f"Last Updated: {policy_manager.get_policy_last_updated()}")

    query = "è¿è´¹æ€ä¹ˆç®—ï¼Ÿ"
    print(f"\nSemantic search for '{query}':")
    results = policy_manager.find_policy_excerpt_semantic(query)
    for res in results:
        print(f"- {res}")

    query = "è´¨é‡é—®é¢˜æ€ä¹ˆåŠï¼Ÿ"
    print(f"\nSemantic search for '{query}':")
    results = policy_manager.find_policy_excerpt_semantic(query)
    for res in results:
        print(f"- {res}")

    query = "ä½ ä»¬å–ä»€ä¹ˆï¼Ÿ" # This query might not match well with policy sentences
    print(f"\nSemantic search for '{query}':")
    results = policy_manager.find_policy_excerpt_semantic(query)
    for res in results:
        print(f"- {res}")

    query = "é€€æ¬¾" # Test keyword fallback
    print(f"\nKeyword search for '{query}':")
    result = policy_manager.find_policy_excerpt([query])
    print(f"- {result}")